{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc116504ac0419ed",
   "metadata": {},
   "source": [
    "The cell below is used to create a Golden Standard csv file, by reading the 'tagged_dataset.json.  Used to check if the model's NER prediction is correct.\n",
    "\n",
    "Format:\n",
    "| FILE_PATH                                               | LESSOR_NAME      | LESSEE_NAME      | PROPERTY_ADDRESS                                 | LEASE_START_DATE | LEASE_END_DATE   | RENT_AMOUNT | SECURITY_DEPOSIT_AMOUNT |\n",
    "|----------------------------------------------------------|------------------|------------------|--------------------------------------------------|------------------|------------------|-------------|--------------------------|\n",
    "| ./datasets/dataset-master/Lease_Agreement_1.docx   | Ashley Martinez  | Sarah Williams   | 5316 Pine Rd, Franklin, CA 70457                 | May 26, 2025     | May 26, 2026     | $1038       |  $1245                    |\n",
    "| ./datasets/dataset-master/Lease_Agreement_2.docx  | Ashley Jones     | Jessica Miller   | 538 Spruce Ct, Springfield, NY 82660             | December 16, 2024| December 16, 2025| $1746       |  $2095                    |\n",
    "| ./datasets/dataset-master/Lease_Agreement_3.docx   | Brian Miller     | Amanda Garcia    | 1807 Chestnut Blvd, Fairview, CA 68967           | September 20, 2024| September 20, 2025| $2611     |  $3133                    |\n",
    "| ./datasets/dataset-master/Lease_Agreement_4.docx   | David Hernandez  | Michael Johnson  | 2658 Elm St, Franklin, GA 71686                  | May 28, 2025     | May 28, 2026     | $3330       |  $3996                   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8b6a180206652f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T17:22:14.986672Z",
     "start_time": "2025-08-03T17:22:14.971863Z"
    }
   },
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "import pandas as pd\n",
    "\n",
    "dataframe = pd.DataFrame(columns=['FILE_PATH', 'LESSOR_NAME', 'LESSEE_NAME', 'PROPERTY_ADDRESS', 'LEASE_START_DATE', 'LEASE_END_DATE', 'RENT_AMOUNT', 'SECURITY_DEPOSIT_AMOUNT'])\n",
    "\n",
    "def data_preprocessing(data):\n",
    "    for f in data:\n",
    "        file_name = f[\"file_path\"]\n",
    "        doc_path = file_name\n",
    "        doc = Document(doc_path)\n",
    "        text = \"\\n\".join([p.text for p in doc.paragraphs if p.text.strip()]).strip()\n",
    "\n",
    "        # Tokenize by whitespace, and track char positions\n",
    "        tokens = []\n",
    "        token_spans = []\n",
    "        start = 0\n",
    "        for word in text.split():\n",
    "            start = text.find(word, start)  # Find next occurrence\n",
    "            end = start + len(word)\n",
    "            tokens.append(word)\n",
    "            token_spans.append((start, end))\n",
    "            start = end\n",
    "\n",
    "        # Initialize all labels to 0 (\"O\")\n",
    "        labels = [0] * len(tokens)\n",
    "        row =[file_name]\n",
    "        # Map character spans to token indices\n",
    "        for entity_name, span in f[\"entities\"].items():\n",
    "            ent_start = span[\"start\"]\n",
    "            ent_end = span[\"end\"]\n",
    "            row.append(text[ent_start:ent_end])\n",
    "        dataframe.loc[len(dataframe)] = row\n",
    "\n",
    "    # Save to CSV\n",
    "    dataframe.to_csv(\"golden_data.csv\", index=False)\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "8e8b6a180206652f",
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa02b84612008c34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T17:22:16.837605Z",
     "start_time": "2025-08-03T17:22:16.284723Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"./tagged_testing_dataset.json\"\n",
    "\n",
    "with open(file_path) as f:\n",
    "    data = json.load(f)\n",
    "    data_preprocessing(data)"
   ],
   "id": "fa02b84612008c34",
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "174d68618f1e0365",
   "metadata": {},
   "source": [
    "### exact_match_results(pred_file, golden_file, result_file='./result.csv')\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "- `pred_file`: Path to the **prediction result CSV file**\n",
    "- `golden_file`: Path to the **golden standard (ground truth) CSV file**\n",
    "- `result_file`: Path to save the output result CSV. Defaults to `'./result.csv'`.\n",
    "\n",
    "---\n",
    "\n",
    "**What it does:**\n",
    "\n",
    "1. Loads both CSVs into Pandas DataFrames.\n",
    "2. Iterates through each row in `pred_file`.\n",
    "3. Matches the corresponding row in `golden_file` using the `FILE_PATH` column.\n",
    "4. Compares each field defined in the evaluation (e.g., `LESSOR_NAME`, `LEASE_START_DATE`, etc.).\n",
    "5. For each field:\n",
    "   - Appends `1` if the prediction **exactly matches** the golden truth.\n",
    "   - Appends `0` if it **does not match**.\n",
    "6. Appends the comparison results as a new row into the result DataFrame.\n",
    "7. Saves the result DataFrame to `result_file` in CSV format.\n",
    "\n",
    "---\n",
    "\n",
    "**Output:**\n",
    "\n",
    "A CSV file where each row contains binary (1/0) match scores for each entity field, representing **exact match accuracy** per field per document.\n",
    "\n",
    "| FILE_PATH                       | LESSOR_NAME | LESSEE_NAME | ... |\n",
    "|--------------------------------|-------------|-------------|-----|\n",
    "| Lease_Agreement_Test3.docx     | 1           | 0           | ... |\n",
    "| Lease_Agreement_Test4.docx     | 1           | 1           | ... |\n",
    "\n",
    "---\n",
    "\n",
    "**Use case:**\n",
    "\n",
    "Best suited for evaluating **NER predictions** when **strict exact match evaluation** is required for each entity.\n"
   ],
   "id": "174d68618f1e0365"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebcff02c2c3851bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T17:21:35.733536Z",
     "start_time": "2025-08-03T17:21:35.664790Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def exact_match_results(pred_file, golden_file, result_file='result.csv'):\n",
    "    # Load prediction and original CSVs\n",
    "    pred_df = pd.read_csv(pred_file)\n",
    "    orig_df = pd.read_csv(golden_file)\n",
    "\n",
    "    # Columns to compare\n",
    "    columns = [\n",
    "        'LESSOR_NAME', 'LESSEE_NAME', 'PROPERTY_ADDRESS',\n",
    "        'LEASE_START_DATE', 'LEASE_END_DATE',\n",
    "        'RENT_AMOUNT', 'SECURITY_DEPOSIT_AMOUNT'\n",
    "    ]\n",
    "\n",
    "    # Initialize the result DataFrame\n",
    "    result_dataframe = pd.DataFrame(columns=['FILE_PATH'] + columns)\n",
    "\n",
    "    # Iterate through prediction file\n",
    "    for i in range(len(pred_df)):\n",
    "        file_path = pred_df.iloc[i].get('FILE_PATH')\n",
    "        orig = orig_df[orig_df['FILE_PATH'] == file_path]\n",
    "\n",
    "        # Skip if no match in original\n",
    "        if orig.empty:\n",
    "            continue\n",
    "\n",
    "        pred_row = pred_df[pred_df['FILE_PATH'] == file_path].iloc[0]\n",
    "        orig_row = orig.iloc[0]\n",
    "\n",
    "        comparison = {'FILE_PATH': file_path}\n",
    "\n",
    "        for col in columns:\n",
    "            if col in pred_row and col in orig_row:\n",
    "                comparison[col] = int(str(pred_row[col]).strip() == str(orig_row[col]).strip())\n",
    "            else:\n",
    "                comparison[col] = \"\"  # or use 0, None, np.nan\n",
    "\n",
    "        result_dataframe = pd.concat([result_dataframe, pd.DataFrame([comparison])], ignore_index=True)\n",
    "\n",
    "    # Save to CSV\n",
    "    result_dataframe.to_csv(result_file, index=False)\n"
   ],
   "id": "ebcff02c2c3851bf",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### partial_match_results(pred_file, golden_file, result_file='partial_result.csv')\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "- `pred_file`: Path to the **prediction result CSV file**\n",
    "- `golden_file`: Path to the **ground truth (golden standard) CSV file**\n",
    "- `result_file`: Path to save the output CSV. Defaults to `'partial_result.csv'`.\n",
    "\n",
    "---\n",
    "\n",
    "**What it does:**\n",
    "\n",
    "1. Loads both CSV files into Pandas DataFrames.\n",
    "2. Iterates through each row in the `pred_file`.\n",
    "3. Matches the row in `golden_file` based on the `FILE_PATH` column.\n",
    "4. For each entity field (e.g., `LESSOR_NAME`, `PROPERTY_ADDRESS`):\n",
    "   - Applies **fuzzy string matching** using `fuzz.token_sort_ratio`.\n",
    "   - Assigns a partial credit score based on match closeness:\n",
    "     - `1.0` if score ≥ 95\n",
    "     - `0.75` if score ≥ 85\n",
    "     - `0.5` if score ≥ 70\n",
    "     - `0.25` if score ≥ 50\n",
    "     - `0.0` otherwise or if missing\n",
    "5. Builds a result DataFrame with per-field fuzzy match scores.\n",
    "6. Saves the output DataFrame to a CSV file at `result_file`.\n",
    "\n",
    "---\n",
    "\n",
    "**Output:**\n",
    "A CSV file where each row represents a lease document, and each column represents a field's **partial match score** between prediction and ground truth.\n",
    "\n",
    "| FILE_PATH                       | LESSOR_NAME | LESSEE_NAME | ... |\n",
    "|--------------------------------|-------------|-------------|-----|\n",
    "| Lease_Agreement_Test10.docx    | 1.0         | 0.75        | ... |\n",
    "\n",
    "---\n",
    "\n",
    "**Use case:**\n",
    "Useful for evaluating **Named Entity Recognition (NER)** outputs where predictions may not be exact matches but are **close enough** semantically or structurally.\n"
   ],
   "id": "4e88dac0d2cc4776"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "525bfd63bb9496c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T17:22:25.123615Z",
     "start_time": "2025-08-03T17:22:25.101796Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def partial_match_results(pred_file, golden_file, result_file='partial_result.csv'):\n",
    "    pred_df = pd.read_csv(pred_file)\n",
    "    orig_df = pd.read_csv(golden_file)\n",
    "\n",
    "    columns = [\n",
    "        'LESSOR_NAME', 'LESSEE_NAME', 'PROPERTY_ADDRESS',\n",
    "        'LEASE_START_DATE', 'LEASE_END_DATE',\n",
    "        'RENT_AMOUNT', 'SECURITY_DEPOSIT_AMOUNT'\n",
    "    ]\n",
    "\n",
    "    result_dataframe = pd.DataFrame(columns=['FILE_PATH'] + columns)\n",
    "\n",
    "    def fuzzy_score(pred, gold):\n",
    "        if pd.isna(pred) or pd.isna(gold):\n",
    "            return 0.0\n",
    "        pred = str(pred).strip()\n",
    "        gold = str(gold).strip()\n",
    "        score = fuzz.token_sort_ratio(pred, gold)\n",
    "        if score >= 95:\n",
    "            return 1.0\n",
    "        elif score >= 85:\n",
    "            return 0.75\n",
    "        elif score >= 70:\n",
    "            return 0.5\n",
    "        elif score >= 50:\n",
    "            return 0.25\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    for i in range(len(pred_df)):\n",
    "        file_path = pred_df.iloc[i].get('FILE_PATH')\n",
    "        orig = orig_df[orig_df['FILE_PATH'] == file_path]\n",
    "\n",
    "        if orig.empty:\n",
    "            continue\n",
    "\n",
    "        pred_row = pred_df[pred_df['FILE_PATH'] == file_path].iloc[0]\n",
    "        orig_row = orig.iloc[0]\n",
    "\n",
    "        comparison = {'FILE_PATH': file_path}\n",
    "        for col in columns:\n",
    "            if col in pred_row and col in orig_row:\n",
    "                comparison[col] = fuzzy_score(pred_row[col], orig_row[col])\n",
    "            else:\n",
    "                comparison[col] = \"\"  # or use 0.0 or None depending on your needs\n",
    "\n",
    "        result_dataframe = pd.concat([result_dataframe, pd.DataFrame([comparison])], ignore_index=True)\n",
    "\n",
    "    result_dataframe.to_csv(result_file, index=False)\n"
   ],
   "id": "525bfd63bb9496c",
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e940662497f432d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T17:22:26.171594Z",
     "start_time": "2025-08-03T17:22:26.105253Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vaishali\\AppData\\Local\\Temp\\ipykernel_47780\\2652686972.py:46: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  result_dataframe = pd.concat([result_dataframe, pd.DataFrame([comparison])], ignore_index=True)\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3af8fe6f1cc9a03d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T17:22:28.633245Z",
     "start_time": "2025-08-03T17:22:28.624853Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_result_csv(result_file, threshold=0.5):\n",
    "    df = pd.read_csv(result_file)\n",
    "\n",
    "    if 'FILE_PATH' in df.columns:\n",
    "        df = df.drop(columns=['FILE_PATH'])\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        values = df[col].fillna(0).astype(float)\n",
    "\n",
    "        tp = (values >= threshold).sum()\n",
    "        total_pred = len(values)\n",
    "        total_gold = len(values)  # assumes gold data is one row per file\n",
    "\n",
    "        precision = tp / total_pred if total_pred else 0\n",
    "        recall = tp / total_gold if total_gold else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "\n",
    "        metrics[col] = {\n",
    "            'total': total_pred,\n",
    "            'true_positives': int(tp),\n",
    "            'precision': round(precision, 3),\n",
    "            'recall': round(recall, 3),\n",
    "            'f1_score': round(f1, 3)\n",
    "        }\n",
    "\n",
    "    return metrics\n"
   ],
   "id": "3af8fe6f1cc9a03d",
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8878649855edf7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T17:22:30.011222Z",
     "start_time": "2025-08-03T17:22:29.992351Z"
    }
   },
   "cell_type": "code",
   "source": "evaluate_result_csv(\"bert_partial_result.csv\")",
   "id": "e8878649855edf7d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LESSOR_NAME': {'total': 14,\n",
       "  'true_positives': 13,\n",
       "  'precision': np.float64(0.929),\n",
       "  'recall': np.float64(0.929),\n",
       "  'f1_score': np.float64(0.929)},\n",
       " 'LESSEE_NAME': {'total': 14,\n",
       "  'true_positives': 13,\n",
       "  'precision': np.float64(0.929),\n",
       "  'recall': np.float64(0.929),\n",
       "  'f1_score': np.float64(0.929)},\n",
       " 'PROPERTY_ADDRESS': {'total': 14,\n",
       "  'true_positives': 9,\n",
       "  'precision': np.float64(0.643),\n",
       "  'recall': np.float64(0.643),\n",
       "  'f1_score': np.float64(0.643)},\n",
       " 'LEASE_START_DATE': {'total': 14,\n",
       "  'true_positives': 5,\n",
       "  'precision': np.float64(0.357),\n",
       "  'recall': np.float64(0.357),\n",
       "  'f1_score': np.float64(0.357)},\n",
       " 'LEASE_END_DATE': {'total': 14,\n",
       "  'true_positives': 10,\n",
       "  'precision': np.float64(0.714),\n",
       "  'recall': np.float64(0.714),\n",
       "  'f1_score': np.float64(0.714)},\n",
       " 'SECURITY_DEPOSIT_AMOUNT': {'total': 14,\n",
       "  'true_positives': 5,\n",
       "  'precision': np.float64(0.357),\n",
       "  'recall': np.float64(0.357),\n",
       "  'f1_score': np.float64(0.357)}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T17:22:31.010728Z",
     "start_time": "2025-08-03T17:22:30.937724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "exact_match_results('spacy_bert_result.csv', 'golden_data.csv', 'spacy_bert_exact_result.csv')\n",
    "partial_match_results('spacy_bert_result.csv', 'golden_data.csv', 'spacy_bert_partial_result.csv')"
   ],
   "id": "b12922b8849d3540",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vaishali\\AppData\\Local\\Temp\\ipykernel_47780\\2652686972.py:46: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  result_dataframe = pd.concat([result_dataframe, pd.DataFrame([comparison])], ignore_index=True)\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T17:22:31.735588Z",
     "start_time": "2025-08-03T17:22:31.724254Z"
    }
   },
   "cell_type": "code",
   "source": "evaluate_result_csv(\"spacy_bert_partial_result.csv\")",
   "id": "1691cb99549a3260",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LESSOR_NAME': {'total': 14,\n",
       "  'true_positives': 14,\n",
       "  'precision': np.float64(1.0),\n",
       "  'recall': np.float64(1.0),\n",
       "  'f1_score': np.float64(1.0)},\n",
       " 'LESSEE_NAME': {'total': 14,\n",
       "  'true_positives': 13,\n",
       "  'precision': np.float64(0.929),\n",
       "  'recall': np.float64(0.929),\n",
       "  'f1_score': np.float64(0.929)},\n",
       " 'PROPERTY_ADDRESS': {'total': 14,\n",
       "  'true_positives': 1,\n",
       "  'precision': np.float64(0.071),\n",
       "  'recall': np.float64(0.071),\n",
       "  'f1_score': np.float64(0.071)},\n",
       " 'LEASE_START_DATE': {'total': 14,\n",
       "  'true_positives': 4,\n",
       "  'precision': np.float64(0.286),\n",
       "  'recall': np.float64(0.286),\n",
       "  'f1_score': np.float64(0.286)},\n",
       " 'LEASE_END_DATE': {'total': 14,\n",
       "  'true_positives': 14,\n",
       "  'precision': np.float64(1.0),\n",
       "  'recall': np.float64(1.0),\n",
       "  'f1_score': np.float64(1.0)},\n",
       " 'SECURITY_DEPOSIT_AMOUNT': {'total': 14,\n",
       "  'true_positives': 1,\n",
       "  'precision': np.float64(0.071),\n",
       "  'recall': np.float64(0.071),\n",
       "  'f1_score': np.float64(0.071)}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "398b315524606e6c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
