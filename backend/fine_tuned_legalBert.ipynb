{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data pre-processing\n",
    "\n",
    "## data_preprocessing(dataset_json):\n",
    "\n",
    "This function processes a dataset of lease agreements for training a token classification (NER) model. Here's what it does step by step:\n",
    "\n",
    "### 1. Loop through each annotated data entry\n",
    "Each item in `dataset_json` is expected to be a dictionary with:\n",
    "- `\"file_path\"`: path to a `.docx` lease agreement\n",
    "- `\"entities\"`: a dictionary of labeled entity spans (e.g., Lessor, Lessee, Address, Rent amount, Start date, End date and Security deposit)\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Read the DOCX file\n",
    "The lease agreement text is extracted from the `.docx` file using `python-docx`.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Tokenize the text (whitespace-based)\n",
    "The text is split into tokens using simple `.split()`.\n",
    "Character spans for each token are tracked (start and end positions in the full string) to later align with entity spans.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Initialize all token labels to 0 (\"O\" = no entity)\n",
    "We create a label list (`labels`) with the same length as tokens. All labels start as 0, indicating \"outside any entity\".\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Align annotated entity spans to tokens\n",
    "For each labeled entity in the annotation:\n",
    "- Get its character start/end position from the JSON\n",
    "- Convert the entity name to an integer ID using `label2id`\n",
    "- For each token, check if the token is **fully inside** the entity span.\n",
    "- If yes, assign the corresponding label ID\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Store the result\n",
    "For each document, a dictionary is created with:\n",
    "- `file_name`: name of the document\n",
    "- `tokens`: list of token strings\n",
    "- `labels`: list of corresponding integer labels\n",
    "\n",
    "This dictionary is added to the `records` list — final output.\n",
    "\n",
    "---\n",
    "\n",
    "### Result\n",
    "You get a list of dictionaries (`records`) that can be used directly for training a transformer-based NER model.\n"
   ],
   "id": "161b3bd7ee3456d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T21:12:59.216999Z",
     "start_time": "2025-08-03T21:12:59.072593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "from docx import Document\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Label map for your entity types\n",
    "label2id = {\n",
    "    \"LESSOR_NAME\": 1,\n",
    "    \"LESSEE_NAME\": 2,\n",
    "    \"PROPERTY_ADDRESS\": 3,\n",
    "    \"LEASE_START_DATE\": 4,\n",
    "    \"LEASE_END_DATE\": 5,\n",
    "    \"RENT_AMOUNT\": 6,\n",
    "    \"SECURITY_DEPOSIT_AMOUNT\": 7\n",
    "}\n",
    "records = []\n",
    "def data_preprocessing(dataset_json):\n",
    "    for data_dic in dataset_json:\n",
    "        doc_path = data_dic[\"file_path\"]  # full path to the DOCX file\n",
    "\n",
    "        # Read the DOCX file\n",
    "        doc = Document(doc_path)\n",
    "        text = \"\\n\".join([p.text for p in doc.paragraphs if p.text.strip()]).strip()\n",
    "\n",
    "        # Tokenize by whitespace, and track char positions\n",
    "        tokens = []\n",
    "        token_spans = []\n",
    "        start = 0\n",
    "        for word in text.split():\n",
    "            start = text.find(word, start)\n",
    "            end = start + len(word)\n",
    "            tokens.append(word)\n",
    "            token_spans.append((start, end))\n",
    "            start = end\n",
    "\n",
    "        # Initialize all labels to 0 (\"O\")\n",
    "        labels = [0] * len(tokens)\n",
    "\n",
    "        # Map character spans to token indices\n",
    "        for entity_name, span in data_dic[\"entities\"].items():\n",
    "            ent_start = span[\"start\"]\n",
    "            ent_end = span[\"end\"]\n",
    "            label_id = label2id[entity_name]\n",
    "\n",
    "            for idx, (tok_start, tok_end) in enumerate(token_spans):\n",
    "                # Check if token overlaps with entity span\n",
    "                if tok_start >= ent_start and tok_end <= ent_end:\n",
    "                    labels[idx] = label_id\n",
    "\n",
    "        # Create a record of data\n",
    "        records.append({\n",
    "            \"file_name\": doc_path,\n",
    "            \"tokens\": tokens,\n",
    "            \"labels\": labels\n",
    "        })\n",
    "\n",
    "\n"
   ],
   "id": "cea452eded90d1c9",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T21:13:01.582340Z",
     "start_time": "2025-08-03T21:13:00.332277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_path = \"./tagged_dataset.json\"\n",
    "\n",
    "import json\n",
    "\n",
    "# Load JSON from file\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    data_preprocessing(data)\n",
    "\n",
    "dataset = pd.DataFrame(records)\n",
    "print(\"Dataset shape: \", dataset.shape)\n"
   ],
   "id": "9ec731f1c116dbc9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape:  (72, 3)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Output of Data pre-processing:",
   "id": "2f473a047b888e6d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T21:01:27.278391Z",
     "start_time": "2025-08-03T21:01:27.238384Z"
    }
   },
   "cell_type": "code",
   "source": "dataset.head()",
   "id": "ae9e4a2e0d4d2b27",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                          file_name  \\\n",
       "0  ./datasets/dataset-master/Lease_Agreement_1.docx   \n",
       "1  ./datasets/dataset-master/Lease_Agreement_2.docx   \n",
       "2  ./datasets/dataset-master/Lease_Agreement_3.docx   \n",
       "3  ./datasets/dataset-master/Lease_Agreement_4.docx   \n",
       "4  ./datasets/dataset-master/Lease_Agreement_5.docx   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [RESIDENTIAL, LEASE, AGREEMENT, This, Lease, A...   \n",
       "1  [RESIDENTIAL, LEASE, AGREEMENT, This, Lease, A...   \n",
       "2  [RESIDENTIAL, LEASE, AGREEMENT, This, Lease, A...   \n",
       "3  [RESIDENTIAL, LEASE, AGREEMENT, This, Lease, A...   \n",
       "4  [RESIDENTIAL, LEASE, AGREEMENT, This, Lease, A...   \n",
       "\n",
       "                                              labels  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./datasets/dataset-master/Lease_Agreement_1.docx</td>\n",
       "      <td>[RESIDENTIAL, LEASE, AGREEMENT, This, Lease, A...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./datasets/dataset-master/Lease_Agreement_2.docx</td>\n",
       "      <td>[RESIDENTIAL, LEASE, AGREEMENT, This, Lease, A...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./datasets/dataset-master/Lease_Agreement_3.docx</td>\n",
       "      <td>[RESIDENTIAL, LEASE, AGREEMENT, This, Lease, A...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./datasets/dataset-master/Lease_Agreement_4.docx</td>\n",
       "      <td>[RESIDENTIAL, LEASE, AGREEMENT, This, Lease, A...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./datasets/dataset-master/Lease_Agreement_5.docx</td>\n",
       "      <td>[RESIDENTIAL, LEASE, AGREEMENT, This, Lease, A...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T21:13:06.064929Z",
     "start_time": "2025-08-03T21:13:06.031613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Saving the prepared data as csv file\n",
    "dataset.to_csv(\"lease_token_labels.csv\", index=False)"
   ],
   "id": "df1434da07602c27",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T21:13:07.874674Z",
     "start_time": "2025-08-03T21:13:07.842469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data to run model\n",
    "df = pd.read_csv(\"./lease_token_labels.csv\")\n"
   ],
   "id": "4f362cbb6f94d83d",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T21:13:10.524105Z",
     "start_time": "2025-08-03T21:13:10.304503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ast\n",
    "\n",
    "# Converting the data to model suitable format.\n",
    "def safe_parse_list(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    return ast.literal_eval(x)\n",
    "\n",
    "df[\"labels\"] = df[\"labels\"].apply(safe_parse_list)\n",
    "df[\"tokens\"] = df[\"tokens\"].apply(safe_parse_list)\n"
   ],
   "id": "5cea6771d0f944a0",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T21:13:12.876688Z",
     "start_time": "2025-08-03T21:13:11.273982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(df[[\"tokens\", \"labels\"]])\n",
    "print(\"Total length of Dataset: \", len(dataset))"
   ],
   "id": "6badf0e2f299b169",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of Dataset:  72\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LegalBERT Model\n",
    "\n",
    "###  Tokenizing and Label Alignment for LegalBERT (NER)\n",
    "\n",
    "We use the `nlpaueb/legal-bert-base-uncased` tokenizer to split words into subwords.\n",
    "\n",
    "However, since our labels are at the **word level**, we must align them with the **tokenized output**, which may split one word into multiple tokens.\n",
    "\n",
    "Key rules:\n",
    "- Only the **first subword** gets the label.\n",
    "- Other subwords get `-100` so that the loss function ignores them.\n",
    "\n",
    "This is handled using `word_ids()` which maps each token back to its source word.\n",
    "\n",
    "We use `.map()` to apply this logic across the entire dataset.\n",
    "\n",
    "\n"
   ],
   "id": "68744df10c8f8898"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T17:17:30.629860Z",
     "start_time": "2025-07-31T17:17:29.850229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
    "\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized = tokenizer(example[\"tokens\"], is_split_into_words=True, truncation=True)\n",
    "    word_ids = tokenized.word_ids()\n",
    "    aligned_labels = []\n",
    "    previous_word = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            aligned_labels.append(-100)\n",
    "        elif word_idx != previous_word:\n",
    "            aligned_labels.append(example[\"labels\"][word_idx])\n",
    "        else:\n",
    "            aligned_labels.append(example[\"labels\"][word_idx] if example[\"labels\"][word_idx] != 0 else -100)\n",
    "        previous_word = word_idx\n",
    "    tokenized[\"labels\"] = aligned_labels\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels)\n"
   ],
   "id": "c72b790b57810209",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/72 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "57a7c165b2d74c83b9bf4d1e87335ff4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T17:17:30.689130Z",
     "start_time": "2025-07-31T17:17:30.676600Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_dataset",
   "id": "a54355dd84f9ac00",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 72\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T17:17:30.776977Z",
     "start_time": "2025-07-31T17:17:30.763571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Splitting dataset into training and validation\n",
    "# 90% train, 10% validation\n",
    "dataset_split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "print(dataset_split.shape)\n"
   ],
   "id": "e0f363c1fd72d660",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': (64, 5), 'test': (8, 5)}\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Load and Configure LegalBERT for NER\n",
    "\n",
    "We use `AutoModelForTokenClassification` to load LegalBERT for a Named Entity Recognition task.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. **Count unique labels** from the dataset and set `num_labels`\n",
    "2. **Load LegalBERT** model with a custom classification head sized to the number of entity types\n",
    "3.  **Define label mappings**:\n",
    "   - `id2label`: maps label indices to names (e.g., `1 → LESSOR_NAME`)\n",
    "   - `label2id`: reverse mapping for training\n",
    "4.  **Inject label maps into the model config** so it can display correct entity names during evaluation\n"
   ],
   "id": "7cf81ea6163f814b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T17:17:31.812826Z",
     "start_time": "2025-07-31T17:17:30.848542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "label_list = list(set(label for example in dataset[\"labels\"] for label in example))\n",
    "num_labels = len(set(label_list))\n",
    "id2label = {i: str(i) for i in range(num_labels)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"nlpaueb/legal-bert-base-uncased\",\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    trust_remote_code=False, use_safetensors=True)\n",
    "model.to(\"cuda\")\n",
    "\n",
    "# Define your label maps\n",
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"LESSOR_NAME\",\n",
    "    2: \"LESSEE_NAME\",\n",
    "    3: \"PROPERTY_ADDRESS\",\n",
    "    4: \"LEASE_START_DATE\",\n",
    "    5: \"LEASE_END_DATE\",\n",
    "    6: \"RENT_AMOUNT\",\n",
    "    7: \"SECURITY_DEPOSIT_AMOUNT\"\n",
    "}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "\n",
    "# Inject into model config\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id"
   ],
   "id": "7f4baab536fc7c39",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### compute_metrics(p): Evaluation Function for Token Classification\n",
    "\n",
    "This function is used by the Hugging Face `Trainer` to evaluate the model's predictions during validation.\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1.  Converts logits into predicted label IDs using `argmax`.\n",
    "2.  Filters out tokens with label `-100` (ignored tokens like padding and subwords).\n",
    "3.  Calculates:\n",
    "   - **Precision**, **Recall**, **F1-score** (weighted average)\n",
    "   - **Accuracy**\n",
    "\n",
    "These metrics are returned as a dictionary and logged during training and evaluation.\n"
   ],
   "id": "955f653c185d6961"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T17:17:31.911971Z",
     "start_time": "2025-07-31T17:17:31.824840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions = np.argmax(p.predictions, axis=2)\n",
    "    labels = p.label_ids\n",
    "\n",
    "    true_preds, true_labels = [], []\n",
    "    for pred, label in zip(predictions, labels):\n",
    "        for p_i, l_i in zip(pred, label):\n",
    "            if l_i != -100:\n",
    "                true_preds.append(p_i)\n",
    "                true_labels.append(l_i)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, true_preds, average='weighted')\n",
    "    acc = accuracy_score(true_labels, true_preds)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n"
   ],
   "id": "b0a95fd38961543f",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### TrainingArguments and Data Collator Setup\n",
    "\n",
    "We configure training using Hugging Face’s `TrainingArguments` for LegalBERT fine-tuning.\n",
    "\n",
    "**Key Settings**:\n",
    "- `eval_strategy=\"epoch\"` → evaluate after every epoch\n",
    "- `save_strategy=\"epoch\"` → save checkpoints after every epoch\n",
    "- `load_best_model_at_end=True` → restore the model with the best validation performance\n",
    "- `fp16=True` → enables mixed precision training for speed/memory efficiency\n",
    "- `save_total_limit=2` → keeps the last 2 checkpoints only\n",
    "\n",
    "We also use `DataCollatorForTokenClassification`, which handles dynamic padding for each batch.\n"
   ],
   "id": "c2555d0110a93cda"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T17:17:32.070582Z",
     "start_time": "2025-07-31T17:17:31.939331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./legalbert-ner-30\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=30,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n"
   ],
   "id": "71c9da4a9020e3e",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Trainer Setup and Training\n",
    "\n",
    "We now initialize Hugging Face's `Trainer` with:\n",
    "\n",
    "- Our `LegalBERT` token classification model\n",
    "- The pre-tokenized training and evaluation datasets\n",
    "- A `data_collator` for dynamic padding\n",
    "- The `compute_metrics()` function for custom evaluation\n",
    "- `TrainingArguments` for logging, saving, and training behavior\n"
   ],
   "id": "f1d81d31890fe35a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T17:20:00.575938Z",
     "start_time": "2025-07-31T17:17:32.111468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_split[\"train\"],\n",
    "    eval_dataset=dataset_split[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ],
   "id": "d3ea023728d7e30a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vaishali\\AppData\\Local\\Temp\\ipykernel_13876\\1209018872.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/240 02:26, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.809053</td>\n",
       "      <td>0.936896</td>\n",
       "      <td>0.879367</td>\n",
       "      <td>0.936896</td>\n",
       "      <td>0.907221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.214261</td>\n",
       "      <td>0.936896</td>\n",
       "      <td>0.877774</td>\n",
       "      <td>0.936896</td>\n",
       "      <td>0.906372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.147940</td>\n",
       "      <td>0.964674</td>\n",
       "      <td>0.931091</td>\n",
       "      <td>0.964674</td>\n",
       "      <td>0.947467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.096714</td>\n",
       "      <td>0.965882</td>\n",
       "      <td>0.939992</td>\n",
       "      <td>0.965882</td>\n",
       "      <td>0.950717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.078332</td>\n",
       "      <td>0.975242</td>\n",
       "      <td>0.970691</td>\n",
       "      <td>0.975242</td>\n",
       "      <td>0.971950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.062715</td>\n",
       "      <td>0.983998</td>\n",
       "      <td>0.982442</td>\n",
       "      <td>0.983998</td>\n",
       "      <td>0.983194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.483100</td>\n",
       "      <td>0.044564</td>\n",
       "      <td>0.989734</td>\n",
       "      <td>0.989553</td>\n",
       "      <td>0.989734</td>\n",
       "      <td>0.989518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.483100</td>\n",
       "      <td>0.036253</td>\n",
       "      <td>0.990942</td>\n",
       "      <td>0.988953</td>\n",
       "      <td>0.990942</td>\n",
       "      <td>0.989533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.483100</td>\n",
       "      <td>0.027879</td>\n",
       "      <td>0.992150</td>\n",
       "      <td>0.992816</td>\n",
       "      <td>0.992150</td>\n",
       "      <td>0.990322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.483100</td>\n",
       "      <td>0.022957</td>\n",
       "      <td>0.993357</td>\n",
       "      <td>0.994669</td>\n",
       "      <td>0.993357</td>\n",
       "      <td>0.993760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.483100</td>\n",
       "      <td>0.023241</td>\n",
       "      <td>0.991244</td>\n",
       "      <td>0.992659</td>\n",
       "      <td>0.991244</td>\n",
       "      <td>0.990106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.483100</td>\n",
       "      <td>0.020604</td>\n",
       "      <td>0.993659</td>\n",
       "      <td>0.994879</td>\n",
       "      <td>0.993659</td>\n",
       "      <td>0.993968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.051700</td>\n",
       "      <td>0.020422</td>\n",
       "      <td>0.993357</td>\n",
       "      <td>0.994580</td>\n",
       "      <td>0.993357</td>\n",
       "      <td>0.993714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.051700</td>\n",
       "      <td>0.014221</td>\n",
       "      <td>0.996679</td>\n",
       "      <td>0.997102</td>\n",
       "      <td>0.996679</td>\n",
       "      <td>0.996790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.051700</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.997283</td>\n",
       "      <td>0.997545</td>\n",
       "      <td>0.997283</td>\n",
       "      <td>0.997351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.051700</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>0.991546</td>\n",
       "      <td>0.993529</td>\n",
       "      <td>0.991546</td>\n",
       "      <td>0.992108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.051700</td>\n",
       "      <td>0.015677</td>\n",
       "      <td>0.994263</td>\n",
       "      <td>0.995148</td>\n",
       "      <td>0.994263</td>\n",
       "      <td>0.994498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.051700</td>\n",
       "      <td>0.020207</td>\n",
       "      <td>0.991546</td>\n",
       "      <td>0.993737</td>\n",
       "      <td>0.991546</td>\n",
       "      <td>0.992227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.016071</td>\n",
       "      <td>0.993056</td>\n",
       "      <td>0.994721</td>\n",
       "      <td>0.993056</td>\n",
       "      <td>0.993583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.017815</td>\n",
       "      <td>0.993961</td>\n",
       "      <td>0.995698</td>\n",
       "      <td>0.993961</td>\n",
       "      <td>0.994635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.015041</td>\n",
       "      <td>0.993961</td>\n",
       "      <td>0.995977</td>\n",
       "      <td>0.993961</td>\n",
       "      <td>0.994720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.016380</td>\n",
       "      <td>0.994867</td>\n",
       "      <td>0.996158</td>\n",
       "      <td>0.994867</td>\n",
       "      <td>0.995351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.020870</td>\n",
       "      <td>0.991848</td>\n",
       "      <td>0.993880</td>\n",
       "      <td>0.991848</td>\n",
       "      <td>0.992650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.015905</td>\n",
       "      <td>0.993357</td>\n",
       "      <td>0.995526</td>\n",
       "      <td>0.993357</td>\n",
       "      <td>0.994160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>0.016418</td>\n",
       "      <td>0.993659</td>\n",
       "      <td>0.995691</td>\n",
       "      <td>0.993659</td>\n",
       "      <td>0.994421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>0.016547</td>\n",
       "      <td>0.993659</td>\n",
       "      <td>0.995691</td>\n",
       "      <td>0.993659</td>\n",
       "      <td>0.994421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>0.017605</td>\n",
       "      <td>0.992754</td>\n",
       "      <td>0.994969</td>\n",
       "      <td>0.992754</td>\n",
       "      <td>0.993566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>0.017288</td>\n",
       "      <td>0.993056</td>\n",
       "      <td>0.995246</td>\n",
       "      <td>0.993056</td>\n",
       "      <td>0.993862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>0.017440</td>\n",
       "      <td>0.992452</td>\n",
       "      <td>0.994688</td>\n",
       "      <td>0.992452</td>\n",
       "      <td>0.993268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>0.017767</td>\n",
       "      <td>0.992452</td>\n",
       "      <td>0.994688</td>\n",
       "      <td>0.992452</td>\n",
       "      <td>0.993268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=240, training_loss=0.1251371572415034, metrics={'train_runtime': 148.0594, 'train_samples_per_second': 12.968, 'train_steps_per_second': 1.621, 'total_flos': 501716987412480.0, 'train_loss': 0.1251371572415034, 'epoch': 30.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T17:20:01.404009Z",
     "start_time": "2025-07-31T17:20:00.685039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Saving the trained model for future use\n",
    "trainer.save_model(\"legalbert-ner-model-30\")\n",
    "tokenizer.save_pretrained(\"legalbert-ner-model-30\")\n"
   ],
   "id": "545fc6ddf348e844",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('legalbert-ner-model-30\\\\tokenizer_config.json',\n",
       " 'legalbert-ner-model-30\\\\special_tokens_map.json',\n",
       " 'legalbert-ner-model-30\\\\vocab.txt',\n",
       " 'legalbert-ner-model-30\\\\added_tokens.json',\n",
       " 'legalbert-ner-model-30\\\\tokenizer.json')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Sample testing:",
   "id": "8e96d9d8c608b721"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T17:20:02.042064Z",
     "start_time": "2025-07-31T17:20:01.434274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner_pipe = pipeline(\"ner\", model=\"legalbert-ner-model-30\", tokenizer=\"legalbert-ner-model-30\",aggregation_strategy=\"simple\")\n",
    "\n",
    "sample_text = \"This Lease Agreement (\\\"Agreement\\\") is entered into on January 1, 2005, by and between: LESSOR: Union Pacific Railroad Company (\\\"Landlord\\\") LESSEE: CXT Incorporated (\\\"Tenant\\\")PROPERTY: The Landlord hereby leases to the Tenant the residential property located at: Grand Island, Nebraska 1. TERM OF LEASE The term of this lease shall commence on January 1, 2005 and shall terminate on December 31, 2009. This Agreement shall be considered a fixed-term lease. 2. RENT The Tenant agrees to pay the Landlord a monthly rent of $1,378. Rent is due on the 1st day of each month. If rent is not received by the 5th day of the month, a late fee of $50.00 will be assessed. 3. SECURITY DEPOSIT Upon execution of this Agreement, Tenant shall deposit with Landlord the sum of $5,000 as a security deposit. This deposit shall be held by the Landlord as security for the faithful performance by the Tenant of all terms, covenants, and conditions of this Agreement.\"\n",
    "result = ner_pipe(sample_text)\n",
    "print(result)\n"
   ],
   "id": "3502827761f36434",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'LEASE_START_DATE', 'score': np.float32(0.74842185), 'word': 'january 1', 'start': 54, 'end': 63}, {'entity_group': 'LEASE_END_DATE', 'score': np.float32(0.38523662), 'word': ',', 'start': 63, 'end': 64}, {'entity_group': 'LEASE_END_DATE', 'score': np.float32(0.40854865), 'word': ',', 'start': 69, 'end': 70}, {'entity_group': 'LESSOR_NAME', 'score': np.float32(0.87431693), 'word': 'union pacific railroad company', 'start': 95, 'end': 125}, {'entity_group': 'LESSEE_NAME', 'score': np.float32(0.9378921), 'word': 'cxt incorporated', 'start': 147, 'end': 163}, {'entity_group': 'PROPERTY_ADDRESS', 'score': np.float32(0.92097855), 'word': ': grand island, nebraska', 'start': 260, 'end': 284}, {'entity_group': 'PROPERTY_ADDRESS', 'score': np.float32(0.5732186), 'word': '.', 'start': 286, 'end': 287}, {'entity_group': 'LEASE_START_DATE', 'score': np.float32(0.8060597), 'word': 'january 1, 2005', 'start': 343, 'end': 358}, {'entity_group': 'LEASE_END_DATE', 'score': np.float32(0.72936994), 'word': 'december 31, 2009.', 'start': 382, 'end': 400}, {'entity_group': 'SECURITY_DEPOSIT_AMOUNT', 'score': np.float32(0.44401678), 'word': '1, 378', 'start': 521, 'end': 526}, {'entity_group': 'LEASE_END_DATE', 'score': np.float32(0.40912125), 'word': ',', 'start': 621, 'end': 622}, {'entity_group': 'SECURITY_DEPOSIT_AMOUNT', 'score': np.float32(0.46905398), 'word': '50. 00', 'start': 638, 'end': 643}, {'entity_group': 'LEASE_END_DATE', 'score': np.float32(0.41219822), 'word': ',', 'start': 714, 'end': 715}, {'entity_group': 'SECURITY_DEPOSIT_AMOUNT', 'score': np.float32(0.7159094), 'word': '$ 5, 000', 'start': 762, 'end': 768}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\NEU\\Summer2025\\NLP\\Assignments\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T17:20:02.076392Z",
     "start_time": "2025-07-31T17:20:02.070072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Beautifying the sample test result\n",
    "def convert_output(ner_results):\n",
    "    entities = []\n",
    "    for ent in ner_results:\n",
    "        if ent[\"entity_group\"] != \"O\":\n",
    "            entities.append({\n",
    "                \"label\": ent[\"entity_group\"],\n",
    "                \"text\": ent[\"word\"],\n",
    "                \"start\": ent[\"start\"],\n",
    "                \"end\": ent[\"end\"],\n",
    "                \"score\": float(ent[\"score\"])\n",
    "            })\n",
    "    return entities\n",
    "\n",
    "# Example usage\n",
    "cleaned_entities = convert_output(result)\n",
    "for ent in cleaned_entities:\n",
    "    print(f\"{ent['label']:25} -> '{ent['text']}' ({ent['start']}-{ent['end']}) [{ent['score']:.2f}]\")\n"
   ],
   "id": "b76dd1c8d44a40fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEASE_START_DATE          -> 'january 1' (54-63) [0.75]\n",
      "LEASE_END_DATE            -> ',' (63-64) [0.39]\n",
      "LEASE_END_DATE            -> ',' (69-70) [0.41]\n",
      "LESSOR_NAME               -> 'union pacific railroad company' (95-125) [0.87]\n",
      "LESSEE_NAME               -> 'cxt incorporated' (147-163) [0.94]\n",
      "PROPERTY_ADDRESS          -> ': grand island, nebraska' (260-284) [0.92]\n",
      "PROPERTY_ADDRESS          -> '.' (286-287) [0.57]\n",
      "LEASE_START_DATE          -> 'january 1, 2005' (343-358) [0.81]\n",
      "LEASE_END_DATE            -> 'december 31, 2009.' (382-400) [0.73]\n",
      "SECURITY_DEPOSIT_AMOUNT   -> '1, 378' (521-526) [0.44]\n",
      "LEASE_END_DATE            -> ',' (621-622) [0.41]\n",
      "SECURITY_DEPOSIT_AMOUNT   -> '50. 00' (638-643) [0.47]\n",
      "LEASE_END_DATE            -> ',' (714-715) [0.41]\n",
      "SECURITY_DEPOSIT_AMOUNT   -> '$ 5, 000' (762-768) [0.72]\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Loading saved model for testing\n",
    "ner_pipe = pipeline(\"ner\", model=\"legalbert-ner-model-100\", tokenizer=\"legalbert-ner-model-100\",\n",
    "                    aggregation_strategy=\"simple\")\n"
   ],
   "id": "9c3a688233a8e820"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Testing the model",
   "id": "3ca88a896a618736"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from docx import Document\n",
    "import pandas as pd\n",
    "\n",
    "dataframe = []\n",
    "\n",
    "\n",
    "def read_docx_directory(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".docx\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                doc = Document(file_path)\n",
    "                text = \"\\n\".join([para.text for para in doc.paragraphs if para.text.strip()])\n",
    "                doc_res = ner_pipe(text)\n",
    "                desired_path = \".\" + file_path[file_path.find(r\"\\datasets\"):]\n",
    "                desired_path = desired_path.replace(\"\\\\\", \"/\")\n",
    "                res = {'FILE_PATH': desired_path, }\n",
    "                for ent in doc_res:\n",
    "                    if ent[\"entity_group\"] != \"O\":\n",
    "                        res[ent[\"entity_group\"]] = ent[\"word\"]\n",
    "                        res[\"score\"] = float(ent[\"score\"])\n",
    "                dataframe.append(res)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {filename}: {e}\")\n",
    "    return pd.DataFrame(dataframe)\n",
    "\n",
    "\n",
    "results = read_docx_directory(\"./datasets/dataset-master/testing\")\n",
    "results.to_csv(\"bert_results.csv\")"
   ],
   "id": "a6ab091c3d054d23"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
